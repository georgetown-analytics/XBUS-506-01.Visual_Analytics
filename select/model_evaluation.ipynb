{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection/Evaluation with Yellowbrick\n",
    "\n",
    "Oftentimes with a new dataset, the choice of the best machine learning algorithm is not always obvious at the outset. Thanks to the scikit-learn API, we can easily approach the problem of model selection using model *evaluation*. As we'll see in these examples, Yellowbrick is helpful for facilitating the process.\n",
    "\n",
    "   \n",
    "\n",
    "## Evaluating Classifiers\n",
    "\n",
    "Classification models attempt to predict a target in a discrete space, that is assign an instance of dependent variables one or more categories. Classification score visualizers display the differences between classes as well as a number of classifier-specific visual evaluations.\n",
    "\n",
    "### ROCAUC\n",
    "\n",
    "A `ROCAUC` (Receiver Operating Characteristic/Area Under the Curve) plot allows the user to visualize the tradeoff between the classifier’s sensitivity and specificity.\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) is a measure of a classifier’s predictive quality that compares and visualizes the tradeoff between the model’s sensitivity and specificity. When plotted, a ROC curve displays the true positive rate on the Y axis and the false positive rate on the X axis on both a global average and per-class basis. The ideal point is therefore the top-left corner of the plot: false positives are zero and true positives are one.\n",
    "\n",
    "This leads to another metric, area under the curve (AUC), which is a computation of the relationship between false positives and true positives. The higher the AUC, the better the model generally is. However, it is also important to inspect the “steepness” of the curve, as this describes the maximization of the true positive rate while minimizing the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "\n",
    "\n",
    "# Load the classification data set\n",
    "X, y = load_occupancy()\n",
    "\n",
    "# Specify the classes of the target\n",
    "classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Instantiate the visualizer with the classification model\n",
    "visualizer = ROCAUC(LogisticRegression(\n",
    "    multi_class=\"auto\", solver=\"liblinear\"\n",
    "    ), classes=classes, size=(1080, 720)\n",
    ")\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellowbrick’s `ROCAUC` Visualizer also allows for plotting **multiclass** classification curves. ROC curves are typically used in binary classification, and in fact the Scikit-Learn `roc_curve` metric is only able to perform metrics for binary classifiers. Yellowbrick addresses this by binarizing the output (per-class) or to use one-vs-rest (micro score) or one-vs-all (macro score) strategies of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "\n",
    "from yellowbrick.datasets import load_game\n",
    "\n",
    "\n",
    "# Load multi-class classification dataset\n",
    "X, y = load_game()\n",
    "classes = ['win', 'loss', 'draw']\n",
    "\n",
    "# Encode the non-numeric columns\n",
    "X = OrdinalEncoder().fit_transform(X)\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")\n",
    "\n",
    "visualizer = ROCAUC(\n",
    "    RidgeClassifier(), classes=classes, size=(1080, 720)\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClassificationReport Heatmap\n",
    "\n",
    "The classification report visualizer displays the precision, recall, F1, and support scores for the model. In order to support easier interpretation and problem detection, the report integrates numerical scores with a color-coded heatmap. All heatmaps are in the range `(0.0, 1.0)` to facilitate easy comparison of classification models across different classification reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "# Load the classification data set\n",
    "X, y = load_occupancy()\n",
    "classes = [\"unoccupied\", \"occupied\"]\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2\n",
    ")\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "bayes = GaussianNB()\n",
    "visualizer = ClassificationReport(\n",
    "    bayes, classes=classes, support=True, size=(1080, 720)\n",
    ")\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report shows a representation of the main classification metrics on a per-class basis. This gives a deeper intuition of the classifier behavior over global accuracy which can mask functional weaknesses in one class of a multiclass problem. Visual classification reports are used to compare classification models to select models that are “redder”, e.g. have stronger classification metrics or that are more balanced.\n",
    "\n",
    "The metrics are defined in terms of true and false positives, and true and false negatives. Positive and negative in this case are generic names for the classes of a binary classification problem. In the example above, we would consider true and false occupied and true and false unoccupied. Therefore a true positive is when the actual class is positive as is the estimated class. A false positive is when the actual class is negative but the estimated class is positive. Using this terminology the meterics are defined as follows:\n",
    "\n",
    "**precision**    \n",
    "    Precision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”\n",
    "\n",
    "**recall**     \n",
    "    Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”\n",
    "\n",
    "**f1 score**     \n",
    "    The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
    "\n",
    "**support**     \n",
    "    Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process.\n",
    "\n",
    "\n",
    "### ClassPredictionError\n",
    "\n",
    "The Yellowbrick `ClassPredictionError` plot is a twist on other and sometimes more familiar classification model diagnostic tools like the Confusion Matrix and Classification Report. Like the Classification Report, this plot shows the support (number of training samples) for each class in the fitted classification model as a stacked bar chart. Each bar is segmented to show the proportion of predictions (including false negatives and false positives, like a Confusion Matrix) for each class. You can use a `ClassPredictionError` to visualize which classes your classifier is having a particularly difficult time with, and more importantly, what incorrect answers it is giving on a per-class basis. This can often enable you to better understand strengths and weaknesses of different models and particular challenges unique to your dataset.\n",
    "\n",
    "The class prediction error chart provides a way to quickly understand how good your classifier is at predicting the right classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from yellowbrick.classifier import ClassPredictionError\n",
    "from yellowbrick.datasets import load_credit\n",
    "\n",
    "\n",
    "X, y = load_credit()\n",
    "classes = ['account in default', 'current with bills']\n",
    "\n",
    "# Perform 80/20 training/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42\n",
    ")\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(\n",
    "    RandomForestClassifier(n_estimators=10), \n",
    "    classes=classes, size=(1080, 720)\n",
    ")\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Regressors\n",
    "\n",
    "Regression models attempt to predict a target in a continuous space. Regressor score visualizers display the instances in model space to better understand how the model is making predictions. \n",
    "\n",
    "### PredictionError\n",
    "\n",
    "A prediction error plot shows the actual targets from the dataset against the predicted values generated by our model. This allows us to see how much variance is in the model. Data scientists can diagnose regression models using this plot by comparing against the 45 degree line, where the prediction exactly matches the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from yellowbrick.regressor import PredictionError\n",
    "from yellowbrick.datasets import load_concrete\n",
    "\n",
    "# Load regression dataset\n",
    "X, y = load_concrete()\n",
    "\n",
    "# Create the train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = Lasso()\n",
    "visualizer = PredictionError(model, size=(1080, 720))\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Plot\n",
    "\n",
    "Residuals, in the context of regression models, are the difference between the observed value of the target variable (y) and the predicted value (ŷ), i.e. the error of the prediction. The residuals plot shows the difference between residuals on the vertical axis and the dependent variable on the horizontal axis, allowing you to detect regions within the target that may be susceptible to more or less error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "\n",
    "# Instantiate the linear model and visualizer\n",
    "model = Ridge()\n",
    "visualizer = ResidualsPlot(model, size=(1080, 720))\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "visualizer.show()                 # Draw the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "\n",
    "regressors = {\n",
    "    \"support vector machine\": SVR(),\n",
    "    \"multilayer perceptron\": MLPRegressor(),\n",
    "    \"nearest neighbors\": KNeighborsRegressor(),\n",
    "    \"bayesian ridge\": BayesianRidge(),\n",
    "    \"linear regression\": LinearRegression(),\n",
    "}\n",
    "\n",
    "for _, regressor in regressors.items():\n",
    "    visualizer = ResidualsPlot(regressor)\n",
    "    visualizer.fit(X_train, y_train)\n",
    "    visualizer.score(X_test, y_test)\n",
    "    visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Target visualizers specialize in visually describing the dependent variable for supervised modeling, often referred to as y or the target.\n",
    "\n",
    "### Class Balance Report\n",
    "\n",
    "One of the biggest challenges for classification models is an imbalance of classes in the training data. Severe class imbalances may be masked by relatively good F1 and accuracy scores – the classifier is simply guessing the majority class and not making any evaluation on the underrepresented class.\n",
    "\n",
    "There are several techniques for dealing with class imbalance such as stratified sampling, down sampling the majority class, weighting, etc. But before these actions can be taken, it is important to understand what the class balance is in the training data. The `ClassBalance` visualizer supports this by creating a bar chart of the support for each class, that is the frequency of the classes’ representation in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.target import ClassBalance\n",
    "\n",
    "# Load multi-class classification dataset\n",
    "X, y = load_game()\n",
    "\n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(\n",
    "    labels=[\"draw\", \"loss\", \"win\"], size=(1080, 720)\n",
    ")\n",
    "\n",
    "visualizer.fit(y)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yellowbrick visualizers are intended to steer the model selection process. Generally, model selection is a search problem defined as follows: given N instances described by numeric properties and (optionally) a target for estimation, find a model described by a triple composed of features, an algorithm and hyperparameters that best fits the data. For most purposes the “best” triple refers to the triple that receives the best cross-validated score for the model type.\n",
    "\n",
    "The yellowbrick.model_selection package provides visualizers for inspecting the performance of cross validation and hyper parameter tuning. Many visualizers wrap functionality found in `sklearn.model_selection` and others build upon it for performing multi-model comparisons.\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "Generally we determine whether a given model is optimal by looking at it’s F1, precision, recall, and accuracy (for classification), or it’s coefficient of determination (R2) and error (for regression). However, real world data is often distributed somewhat unevenly, meaning that the fitted model is likely to perform better on some sections of the data than on others. Yellowbrick’s `CVScores` visualizer enables us to visually explore these variations in performance using different cross validation strategies.\n",
    "\n",
    "Cross-validation starts by shuffling the data (to prevent any unintentional ordering errors) and splitting it into `k` folds. Then `k` models are fit on $\\frac{k-1} {k}$ of the data (called the training split) and evaluated on $\\frac {1} {k}$ of the data (called the test split). The results from each evaluation are averaged together for a final score, then the final model is fit on the entire dataset for operationalization.\n",
    "\n",
    "In Yellowbrick, the `CVScores` visualizer displays cross-validated scores as a bar chart (one bar for each fold) with the average score across all folds plotted as a horizontal dotted line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from yellowbrick.model_selection import CVScores\n",
    "\n",
    "# Load the classification data set\n",
    "X, y = load_occupancy()\n",
    "\n",
    "# Create a cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=12)\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "model = MultinomialNB()\n",
    "visualizer = CVScores(\n",
    "    model, cv=cv, scoring='f1_weighted', size=(1080, 720)\n",
    ")\n",
    "\n",
    "visualizer.fit(X, y)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit the Yellowbrick docs for more about visualizers for [classification](http://www.scikit-yb.org/en/latest/api/classifier/index.html), [regression](http://www.scikit-yb.org/en/latest/api/regressor/index.html) and [model selection](http://www.scikit-yb.org/en/latest/api/model_selection/index.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
