{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to compare models for machine learning\n",
    "\n",
    "In this lab, we are going to look at scores for a variety of models and compare them in order to select the best model for our data. We will build two pipelines- one that runs just a selected estimator and one that will include a SelectFromModel step prior to modeling our data. We will write our scores to a file that we can use for creating plots comparing the models.\n",
    "\n",
    "This lab borrows heavily from the [Census notebook](https://github.com/georgetown-analytics/machine-learning/tree/master/examples/bbengfort/census) we reviewed at the end of the machine learning class. We did this to reinforce the concepts we went over in Machine Learning and to show you another way to adapt that code to different data. We are going to use a different UCI data set in this exercise-- the Mushroom Data Set. Our objective is to predict if a mushroom is poisionous or edible based on its characteristics. \n",
    "\n",
    "Sources:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom\n",
    "\n",
    "https://github.com/gumption/Python_for_Data_Science\n",
    "\n",
    "https://github.com/georgetown-analytics/machine-learning/tree/master/examples/bbengfort/census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.datasets.base import Bunch\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV, LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, auc, roc_curve, roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "We are going to use a **modified** version of the [Mushroom Dataset](https://archive.ics.uci.edu/ml/datasets/Mushroom) from the UCI machine learning library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We have provided two files- agaricus-lepiota.data, and agaricus-lepiota.names. The names file provides us with the following information:\n",
    "\n",
    "Relevant Information:\n",
    "    This data set includes descriptions of hypothetical samples\n",
    "    corresponding to 23 species of gilled mushrooms in the Agaricus and\n",
    "    Lepiota Family (pp. 500-525).  Each species is identified as\n",
    "    definitely edible, definitely poisonous, or of unknown edibility and\n",
    "    not recommended.  This latter class was combined with the poisonous\n",
    "    one.  The Guide clearly states that there is no simple rule for\n",
    "    determining the edibility of a mushroom; no rule like ``leaflets\n",
    "    three, let it be'' for Poisonous Oak and Ivy.\n",
    "\n",
    "Number of Instances: 8124\n",
    "\n",
    "Number of Attributes: 22 (all nominally valued)\n",
    "\n",
    "**However**, for this exercise, we're going to use the class labels and the first three attributes.\n",
    "\n",
    "Attribute Information: (classes: edible=e, poisonous=p)\n",
    "     1. cap-shape:                bell=b,conical=c,convex=x,flat=f,\n",
    "                                  knobbed=k,sunken=s\n",
    "     2. cap-surface:              fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "     3. cap-color:                brown=n,buff=b,cinnamon=c,gray=g,green=r,\n",
    "                                  pink=p,purple=u,red=e,white=w,yellow=y\n",
    "\n",
    "\n",
    "Class Distribution: \n",
    "    --    edible: 4208 (51.8%)\n",
    "    -- poisonous: 3916 (48.2%)\n",
    "    --     total: 8124 instances           \n",
    "    \n",
    "\n",
    "Let's load the data with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    'class',\n",
    "    'cap-shape',\n",
    "    'cap-surface',\n",
    "    'cap-color'\n",
    "]\n",
    "\n",
    "mushrooms = os.path.join('data','mushrooms','agaricus-lepiota.data')\n",
    "data = pd.read_csv(mushrooms, usecols=['p','x','s','n'])\n",
    "data.columns = names\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data, including the target, is categorical. We will need to change these values to numeric ones for machine learning.\n",
    "\n",
    "But first, let's look at some plots of our features. Using what we learned about visualizations with Seaborn, we can quickly plot whether a mushroom is edible or poisonous by each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#seaborn factorplot to show edible/poisonous breakdown by different factors\n",
    "for name in names[1:]:\n",
    "    g = sns.factorplot(\"class\", col=name, data=data,\n",
    "                    kind=\"count\", size=2.5, aspect=.8, col_wrap=7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factorplots show us how many mushrooms with each feature value are poisionous or edible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management\n",
    "\n",
    "We need to structure our data on disk in a way that we can load into Scikit-Learn in a repeatable fashion for continued analysis. As we have discussed before, we can use sklearn.datasets.base.Bunch object to load the data into data and target attributes respectively, similar to how Scikit-Learn's toy datasets are structured (see Machine Learning lectures and Census notebook). \n",
    "\n",
    "In order to organize our data on disk, we'll need to add the following files:\n",
    "\n",
    "README.md: a markdown file containing information about the dataset and attribution. Will be exposed by the DESCR attribute.\n",
    "\n",
    "meta.json: a helper file that contains machine readable information about the dataset like target_names and feature_names.\n",
    "\n",
    "The README.md file can be created directly using a text editor. Create your file with a link to the data set and some information on the source of the data as listed on the UCI page. For example, you could use the following information in your README file:\n",
    "\n",
    "# Mushroom Data Set\n",
    "\n",
    "Data downloaded from [UCI Machine Learning Repository- Mushroom Data Set](http://archive.ics.uci.edu/ml/machine-learning-databases/mushroom).\n",
    "\n",
    "## Data Set Information\n",
    "\n",
    "Origin: \n",
    "\n",
    "Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf \n",
    "\n",
    "Donor: \n",
    "\n",
    "Jeff Schlimmer (Jeffrey.Schlimmer '@' a.gp.cs.cmu.edu)\n",
    "\n",
    "Prediction task is to determine whether a mushroom is edible or poisonous.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We can write the meta.json file using the data frame that we already have. However, as we saw in the Census example, it is useful to obtain the unique values of categorical data when we have categorical values. It would be more meaningful if the attribute values were full words instead of single letters. \n",
    "\n",
    "In order to facilitate this, create a file in your text editor called 'agaricus-lepiota.attributes' with the attribute information from the .names file and store it with your data. We will use this to create a dictionary of values for every attribute which we can use to map our values. Copy the attribues information from the .names file and edit the file so each line of the text file contains one attribute and its values. For example:\n",
    "   \n",
    "    class: edible=e, poisonous=p\n",
    "    cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
    "    cap-surface: fibrous=f, grooves=g, scaly=y, smooth=s\n",
    "    ...\n",
    "\n",
    "When your file is complete, you can review the contents of the file in your notebook using the command below.\n",
    "\n",
    "(! is a [cell magics](https://ipython.org/ipython-doc/dev/interactive/magics.html) that allows us to make calls to the shell. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the command appropriate for your OS.\n",
    "\n",
    "# OSX/ Linux\n",
    "! cat data/mushrooms/agaricus-lepiota.attributes \n",
    "\n",
    "# Windows\n",
    "#! type data\\mushrooms\\agaricus-lepiota.attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this file to create a list of dictionaries. This list will allow us to easily replace the single letters with the full word it represents.\n",
    "\n",
    "Luckily, someone else has done the hardwork on this for us. The code below comes from [Python for Data Science](https://github.com/gumption/Python_for_Data_Science)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_attribute_names_and_values(filename):\n",
    "    '''Returns a list of attribute names and values in a file.\n",
    "    \n",
    "    This list contains dictionaries wherein the keys are names \n",
    "    and the values are value description dictionariess.\n",
    "    \n",
    "    Each value description sub-dictionary will use \n",
    "    the attribute value abbreviations as its keys \n",
    "    and the attribute descriptions as the values.\n",
    "    \n",
    "    filename is expected to have one attribute name and set of values per line, \n",
    "    with the following format:\n",
    "        name: value_description=value_abbreviation[,value_description=value_abbreviation]*\n",
    "    for example\n",
    "        cap-shape: bell=b, conical=c, convex=x, flat=f, knobbed=k, sunken=s\n",
    "    The attribute name and values dictionary created from this line would be the following:\n",
    "        {'name': 'cap-shape', \n",
    "         'values': {'c': 'conical', \n",
    "                    'b': 'bell', \n",
    "                    'f': 'flat', \n",
    "                    'k': 'knobbed', \n",
    "                    's': 'sunken', \n",
    "                    'x': 'convex'}}\n",
    "    '''\n",
    "    attribute_names_and_values = []  # this will be a list of dicts\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            attribute_name_and_value_dict = {}\n",
    "            attribute_name_and_value_string_list = line.strip().split(':')\n",
    "            attribute_name = attribute_name_and_value_string_list[0]\n",
    "            attribute_name_and_value_dict['name'] = attribute_name\n",
    "            if len(attribute_name_and_value_string_list) < 2:\n",
    "                attribute_name_and_value_dict['values'] = None # no values for this attribute\n",
    "            else:\n",
    "                value_abbreviation_description_dict = {}\n",
    "                description_and_abbreviation_string_list = attribute_name_and_value_string_list[1].strip().split(',')\n",
    "                for description_and_abbreviation_string in description_and_abbreviation_string_list:\n",
    "                    description_and_abbreviation = description_and_abbreviation_string.strip().split('=')\n",
    "                    description = description_and_abbreviation[0]\n",
    "                    if len(description_and_abbreviation) < 2: # assumption: no more than 1 value is missing an abbreviation\n",
    "                        value_abbreviation_description_dict[None] = description\n",
    "                    else:\n",
    "                        abbreviation = description_and_abbreviation[1]\n",
    "                        value_abbreviation_description_dict[abbreviation] = description\n",
    "                attribute_name_and_value_dict['values'] = value_abbreviation_description_dict\n",
    "            attribute_names_and_values.append(attribute_name_and_value_dict)\n",
    "    return attribute_names_and_values\n",
    "\n",
    "\n",
    "attribute_filename = os.path.join('data','mushrooms','agaricus-lepiota.attributes')\n",
    "attribute_names_and_values = load_attribute_names_and_values(attribute_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list that contains a dictionary for each column in our data. Take a look at the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attribute_names_and_values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these dictionaries to replace our single letters with the full word it stands for and write our new dataframe to a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What does the while loop below do? Explain the code to your partner in plain english. Add extra cells to the notebook\n",
    "# to test pieces of the code if that helps.\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(names):\n",
    "    data.replace({names[i] : attribute_names_and_values[i]['values']}, inplace=True)\n",
    "    i += 1\n",
    "\n",
    "data.to_csv(os.path.join('data', 'mushrooms', 'agaricus-lepiota.txt'), header=None, index=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our meta.json file and bunches.\n",
    "\n",
    "We are going to reuse code from the [Census notebook](https://github.com/georgetown-analytics/machine-learning/tree/master/examples/bbengfort/census) we discussed in Machine Learning. It has been modified to work with our data here.\n",
    "\n",
    "This code creates a `meta.json` file by inspecting the data frame that we have constructued. The `target_names` column, is just the two unique values in the `data['class']` series; by using the `pd.Series.unique` method - we're guarenteed to spot data errors if there are more or less than two values. The `feature_names` is simply the names of all the columns. \n",
    "\n",
    "Then we get tricky &mdash; we want to store the possible values of each categorical field for lookup later, but how do we know which columns are categorical and which are not? Luckily, Pandas has already done an analysis for us, and has stored the column data type, `data[column].dtype`, as either `int64` or `object`. Here I am using a dictionary comprehension to create a dictionary whose keys are the categorical columns, determined by checking the object type and comparing with `object`, and whose values are a list of unique values for that field. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_file = os.path.join('data','mushrooms','meta.json')\n",
    "\n",
    "meta = {\n",
    "    'target_names': list(data['class'].unique()),\n",
    "    'feature_names': list(data.columns),\n",
    "    'categorical_features': {\n",
    "        column: list(data[column].unique())\n",
    "        for column in data.columns\n",
    "        if data[column].dtype == 'object'\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(meta, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation or Drop\n",
    "\n",
    "According to the information we received with the dataset, there are missing values in the stalk-root feature. Missing values are indicated with '?', which we replaced with the value 'missing'.\n",
    "\n",
    "Given our lack of subject matter expertise and the numer of instances available, we will drop instances where 'stalk-root' == 'missing'. We want to do that before we create the bunch. If we chose to impute the values, we would pass everything through to the bunch and deal with that value in our pipeline.\n",
    "\n",
    "**Bonus:** At the end of the exercise, create a bunch with the instances we just dropped and run the data again to see what your scores look like if 'missing' is encoded as an accepted value.\n",
    "\n",
    "Now that we have everything we need stored on disk, we can create a `load_data` function, which will allow us to load the training and test datasets appropriately from disk and store them in a `Bunch`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    root = os.path.join('data','mushrooms')\n",
    "    # Load the meta data from the file \n",
    "    with open(os.path.join(root, 'meta.json'), 'r') as f:\n",
    "        meta = json.load(f) \n",
    "    \n",
    "    names = meta['feature_names']\n",
    "    \n",
    "    # Load the readme information \n",
    "    with open(os.path.join(root, 'README.md'), 'r') as f:\n",
    "        readme = f.read() \n",
    "    \n",
    "    # Load the data from the file where we updated the feature values \n",
    "    mushrooms = pd.read_csv(os.path.join(root, 'agaricus-lepiota.txt'), names=names)\n",
    "    \n",
    "    # Remove the target from the categorical features \n",
    "    meta['categorical_features'].pop('class')\n",
    "    \n",
    "    # Return the bunch with the appropriate data chunked apart\n",
    "    return Bunch(\n",
    "        data = mushrooms[names[1:]], \n",
    "        target = mushrooms[names[0]], \n",
    "        target_names = meta['target_names'],\n",
    "        feature_names = meta['feature_names'], \n",
    "        categorical_features = meta['categorical_features'], \n",
    "        DESCR = readme,\n",
    "    )\n",
    "\n",
    "dataset = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the code above to the load_data() function in the Census notebook. What was changed? Why?\n",
    "\n",
    "## Feature Extraction \n",
    "\n",
    "Unfortunately, the categorical values themselves are not useful for machine learning; we need a single instance table that contains _numeric values_. In order to extract this from the dataset, we'll have to use Scikit-Learn transformers to transform our input dataset into something that can be fit to a model. In particular, we'll have to do the following:\n",
    "\n",
    "- encode the categorical labels as numeric data \n",
    "- impute missing values with data (or remove)\n",
    "\n",
    "Our first step is to get our data out of the object data type land and into a numeric type, since nearly all operations we'd like to apply to our data are going to rely on numeric types. Luckily, Sckit-Learn does provide a transformer for converting categorical labels into numeric integers: [`sklearn.preprocessing.LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\n",
    "\n",
    "Unfortunately it can only transform a single vector at a time, so we'll have to adapt it in order to apply it to multiple columns. We can do this a couple of different ways: write code that uses a loop to encode each column or create a multicolumn LabelEncoder.\n",
    "\n",
    "The ability to create custom transformers to use in a pipeline is a powerful skill to have. Let's use the LabelEncoder from the Census notebook and use it with the mushroom data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncodeCategorical(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Encodes a specified list of columns or all columns if None. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, columns=None):\n",
    "        self.columns  = [col for col in columns] \n",
    "        self.encoders = None\n",
    "    \n",
    "    def fit(self, data, target=None):\n",
    "        \"\"\"\n",
    "        Expects a data frame with named columns to encode. \n",
    "        \"\"\"\n",
    "        # Encode all columns if columns is None\n",
    "        if self.columns is None:\n",
    "            self.columns = data.columns \n",
    "        \n",
    "        # Fit a label encoder for each column in the data frame\n",
    "        self.encoders = {\n",
    "            column: LabelEncoder().fit(data[column])\n",
    "            for column in self.columns \n",
    "        }\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"\n",
    "        Uses the encoders to transform a data frame. \n",
    "        \"\"\"\n",
    "        output = data.copy()\n",
    "        for column, encoder in self.encoders.items():\n",
    "            output[column] = encoder.transform(data[column])\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the class as written, we will have the ability to use this to transform all of our features using the LabelEncoder. We will use it as part of our pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's build a way to evaluate multiple estimators, both on their own and using [SelectFromModel](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set some things up for our function\n",
    "\n",
    "if not os.path.exists(os.path.join('data', 'mushrooms', 'output')):\n",
    "                      os.mkdir(os.path.join('data', 'mushrooms', 'output')) \n",
    "                      \n",
    "OUTPATH = os.path.abspath(os.path.join('.', 'data', 'mushrooms', 'output'))\n",
    "print(OUTPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def model_selection(dataset, feature_model, model_estimator, fse_label, model_label):\n",
    "\n",
    "    \"\"\"\n",
    "    Test various combinations of estimators for feature selection and modeling.\n",
    "    The pipeline generates the dataset, encodes columns, then uses the encoded results for feature selection. \n",
    "    Finally,the selected features are sent to the estimator model for scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    start  = time.time()\n",
    "    \n",
    "    # assign X \n",
    "    X = dataset.data\n",
    "    # assign y, encoding the target value\n",
    "    y = LabelEncoder().fit_transform(dataset.target)\n",
    "    \n",
    "    if feature_model == 'none':\n",
    "        # use the pipeline that does not use SelectFromModel\n",
    "        model = Pipeline([\n",
    "                 ('label_encoding', EncodeCategorical(dataset.categorical_features.keys())), \n",
    "                 ('one_hot_encoder', OneHotEncoder()), \n",
    "                 ('estimator', model_estimator)\n",
    "            ])\n",
    "    else:\n",
    "        #use the pipeline that has SelectFromModel\n",
    "        model = Pipeline([\n",
    "                         ('label_encoding', EncodeCategorical()), \n",
    "                         ('one_hot_encoder', OneHotEncoder()), \n",
    "                         ('estimator', model_estimator), \n",
    "                         ('feature_selection', SelectFromModel(feature_model)), \n",
    "                         ('estimator', model_estimator)\n",
    "                          ])\n",
    "\n",
    "    \"\"\"\n",
    "    Train and test the model using StratifiedKFold cross validation. Compile the scores for each iteration of the model.\n",
    "    \"\"\"\n",
    "    scores = {'accuracy':[], 'auc':[], 'f1':[], 'precision':[], 'recall':[]}\n",
    "    \n",
    "    for train, test in StratifiedKFold(y, n_folds=4, shuffle=True):  # Ben says always use 12 folds! We cheat a bit here...\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        expected  = y_test\n",
    "        predicted = model.predict(X_test)\n",
    "        \n",
    "        ## Visualize scores\n",
    "        fpr, tpr, thresholds = roc_curve(expected, predicted)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC-AUC for {}'.format(model_label))\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        ## Record scores\n",
    "        scores['accuracy'].append(accuracy_score(expected, predicted))\n",
    "        scores['f1'].append(f1_score(expected, predicted, average='binary'))\n",
    "        scores['precision'].append(precision_score(expected, predicted, average='binary'))\n",
    "        scores['recall'].append(recall_score(expected, predicted, average='binary'))\n",
    "\n",
    "        \"\"\"\n",
    "        AUC cannot be computed if only 1 class is represented in the data. When that happens, record an AUC score of 0.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scores['auc'].append(roc_auc_score(expected, predicted))\n",
    "        except:\n",
    "            scores['auc'].append(0)\n",
    "\n",
    "    \"\"\"\n",
    "    Print the modeling details and the mean score.\n",
    "    \"\"\"\n",
    "    print(\"\\nBuild and Validation of took {:0.3f} seconds\\n\".format(time.time()-start))\n",
    "    print(\"Feature Selection Estimator: {}\\n\".format(fse_label))\n",
    "    print(\"Estimator Model: {}\\n\".format(model_label))\n",
    "    print(\"Validation scores are as follows:\\n\")\n",
    "    print(pd.DataFrame(scores).mean())\n",
    "\n",
    "    \"\"\"\n",
    "    Create a data frame with the mean score and estimator details.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(scores).mean()\n",
    "    df['SelectFromModel'] =  fse_label\n",
    "    df['Estimator']  = model_label\n",
    "\n",
    "    \"\"\"\n",
    "    Write official estimator to disk\n",
    "    \"\"\"\n",
    "    estimator = model\n",
    "    estimator.fit(X,y)\n",
    "\n",
    "    pickle_path = os.path.join(OUTPATH + \"/\", fse_label.lower().replace(\" \", \"-\") + \"_\" + model_label.lower().replace(\" \", \"-\") + \".pickle\")\n",
    "    with open(pickle_path, 'wb') as f:\n",
    "        pickle.dump(estimator, f)\n",
    "\n",
    "    print(\"\\nFitted model written to:\\n{}\".format(os.path.abspath(pickle_path)))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read through the function and make sure you understand what it is doing. \n",
    "\n",
    "First, it uses the pipeline based on whether or not there is a SelectFromModel estimator provided.\n",
    "\n",
    "It then splits the data runs the model multiple times on different sets of data. It creates a dictionary of scores for each model, printing the mean of each set of scores.\n",
    "\n",
    "Next, the estimator is fit with all the data and saved to a pickle file on your drive.\n",
    "\n",
    "Last, the function returns a dataframe object with the mean scores from modeling and the label for each of the estimators.\n",
    "\n",
    "When we call the function, we are transposing the returned dataframe and appending it to a new dataframe. Once we are done doing the calls we want, we save the final dataframe as a csv file. We can review this manually and also use it to plot information about how our models perform.\n",
    "\n",
    "Below, we are making the calls. Use the examples provided to add additional items to model including calls with an estimator to use for SelectFromModel. As an example:\n",
    "\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, LogisticRegression(), LinearSVC(), \"LogisticRegression\", \"LinearSVC\")).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluation = pd.DataFrame()\n",
    "\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", LogisticRegression(), \"none\", \"LogisticRegression\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", KNeighborsClassifier(), \"none\", \"KNeighborsClassifier\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", BaggingClassifier(KNeighborsClassifier()), \"none\", \"BaggedKNeighborsClassifier\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", RandomForestClassifier(), \"none\", \"RandomForestClassifier\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", ExtraTreesClassifier(), \"none\", \"ExtraTreesClassifier\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", SGDClassifier(), \"none\", \"SGDClassifier\")).T)\n",
    "evaluation = evaluation.append(pd.DataFrame(model_selection(dataset, \"none\", LinearSVC(), \"none\", \"LinearSVC\")).T)\n",
    "\n",
    "evaluation.to_csv(os.path.join('data', 'mushrooms', 'model_comparison.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
